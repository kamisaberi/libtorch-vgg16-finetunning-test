{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T18:57:31.127900Z",
     "start_time": "2025-08-29T18:55:31.556720Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# --- 1. Global Configuration ---\n",
    "config = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"DATA_DIR\": \"/home/kami/Documents/datasets/\",\n",
    "    \"NUM_CLASSES\": 101,\n",
    "    \"EVOLUTION_POPULATION_SIZE\": 10,\n",
    "    \"EVOLUTION_GENERATIONS\": 5,\n",
    "    \"EVOLUTION_NUM_PARENTS\": 3,\n",
    "    \"EVAL_EPOCHS\": 1,\n",
    "}\n",
    "\n",
    "# --- 2. Modular AI Components ---\n",
    "class Permute(nn.Module):\n",
    "    def __init__(self, *dims): super().__init__(); self.dims = dims\n",
    "    def forward(self, x): return x.permute(*self.dims)\n",
    "\n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, dim): super().__init__(); self.dim = dim\n",
    "    def forward(self, x): return x.squeeze(self.dim)\n",
    "\n",
    "class CNNToTransformerAdapter(nn.Module):\n",
    "    def __init__(self, cnn_out_channels, transformer_embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(cnn_out_channels, transformer_embed_dim, kernel_size=1)\n",
    "        self.flatten = nn.Flatten(2)\n",
    "        self.norm = nn.LayerNorm(transformer_embed_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x); x = self.flatten(x); x = x.permute(0, 2, 1); x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class AssembledBackbone(nn.Module):\n",
    "    def __init__(self, genome, module_library):\n",
    "        super().__init__()\n",
    "        self.genome = genome\n",
    "        self.module_list = nn.ModuleList()\n",
    "        self.output_channels = 0\n",
    "\n",
    "        current_channels = 3\n",
    "        current_type = 'image'\n",
    "\n",
    "        for module_name in genome:\n",
    "            module_info = module_library[module_name]\n",
    "            if module_info['input_type'] == 'sequence' and current_type == 'image':\n",
    "                adapter = CNNToTransformerAdapter(current_channels, module_info['input_channels'])\n",
    "                self.module_list.append(adapter)\n",
    "\n",
    "            module_instance = copy.deepcopy(module_info['module'])\n",
    "            self.module_list.append(module_instance)\n",
    "            current_channels = module_info['output_channels']\n",
    "            current_type = module_info['output_type']\n",
    "\n",
    "        if current_type == 'image':\n",
    "            self.module_list.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            self.module_list.append(nn.Flatten(1))\n",
    "            self.output_channels = current_channels\n",
    "        else:\n",
    "            self.module_list.append(Permute(0, 2, 1))\n",
    "            self.module_list.append(nn.AdaptiveAvgPool1d(1))\n",
    "            self.module_list.append(Squeeze(-1))\n",
    "            self.output_channels = current_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# --- 3. The Search System (Evolutionary Algorithm) ---\n",
    "class NeuralArchitectureSearch:\n",
    "    def __init__(self, search_space, module_library, dataset):\n",
    "        self.search_space = search_space\n",
    "        self.module_library = module_library\n",
    "        self.dataset = dataset\n",
    "        self.population = []\n",
    "        self.history = []\n",
    "\n",
    "    def _get_compatible_modules(self, stage, current_type, current_channels):\n",
    "        # Helper function to find valid next modules\n",
    "        possible_modules = []\n",
    "        for m in stage:\n",
    "            module_info = self.module_library[m]\n",
    "            # Case 1: Direct type and channel match\n",
    "            if module_info['input_type'] == current_type and module_info['input_channels'] == current_channels:\n",
    "                possible_modules.append(m)\n",
    "            # Case 2: Adapter can bridge from image to sequence\n",
    "            elif module_info['input_type'] == 'sequence' and current_type == 'image':\n",
    "                possible_modules.append(m)\n",
    "        return possible_modules\n",
    "\n",
    "    def _generate_random_genome(self):\n",
    "        genome = []\n",
    "        current_type = 'image'\n",
    "        current_channels = 3 # Start with RGB image\n",
    "        for stage in self.search_space:\n",
    "            # ================================================================= #\n",
    "            # --- THIS IS THE CORRECTED LOGIC ---\n",
    "            # It now checks both type AND channel compatibility\n",
    "            possible_modules = self._get_compatible_modules(stage, current_type, current_channels)\n",
    "            # ================================================================= #\n",
    "\n",
    "            if not possible_modules:\n",
    "                raise ValueError(f\"Could not find a compatible module for stage. Current state: type={current_type}, channels={current_channels}\")\n",
    "\n",
    "            chosen_module = random.choice(possible_modules)\n",
    "            genome.append(chosen_module)\n",
    "            current_type = self.module_library[chosen_module]['output_type']\n",
    "            current_channels = self.module_library[chosen_module]['output_channels']\n",
    "        return genome\n",
    "\n",
    "    def _mutate_genome(self, genome):\n",
    "        mutation_point = random.randint(0, len(genome) - 1)\n",
    "\n",
    "        if mutation_point == 0:\n",
    "            input_type_needed = 'image'\n",
    "            input_channels_needed = 3\n",
    "        else:\n",
    "            prev_module_info = self.module_library[genome[mutation_point - 1]]\n",
    "            input_type_needed = prev_module_info['output_type']\n",
    "            input_channels_needed = prev_module_info['output_channels']\n",
    "\n",
    "        stage = self.search_space[mutation_point]\n",
    "        compatible_replacements = self._get_compatible_modules(stage, input_type_needed, input_channels_needed)\n",
    "\n",
    "        if not compatible_replacements: # Should not happen with a well-defined search space\n",
    "            return copy.deepcopy(genome) # No valid mutation, return original\n",
    "\n",
    "        new_module_name = random.choice(compatible_replacements)\n",
    "        new_genome = copy.deepcopy(genome)\n",
    "        new_genome[mutation_point] = new_module_name\n",
    "        return new_genome\n",
    "\n",
    "    def _evaluate_genome_with_caft(self, genome):\n",
    "        print(f\"\\nEvaluating Genome: {' -> '.join(genome)}\")\n",
    "\n",
    "        backbone = AssembledBackbone(genome, self.module_library).to(config[\"DEVICE\"])\n",
    "        for param in backbone.parameters(): param.requires_grad = False\n",
    "        backbone.eval()\n",
    "\n",
    "        output_dim = backbone.output_channels\n",
    "        classifier = nn.Linear(output_dim, config[\"NUM_CLASSES\"]).to(config[\"DEVICE\"])\n",
    "        optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        caching_start_time = time.time()\n",
    "        caching_loader = DataLoader(self.dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)\n",
    "        cached_features = torch.zeros(len(self.dataset), output_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            start_idx = 0\n",
    "            for images, _ in caching_loader:\n",
    "                features = backbone(images.to(config[\"DEVICE\"]))\n",
    "                cached_features[start_idx : start_idx + images.size(0)] = features.cpu()\n",
    "                start_idx += images.size(0)\n",
    "        caching_time = time.time() - caching_start_time\n",
    "        cached_features = cached_features.to(config[\"DEVICE\"])\n",
    "\n",
    "        train_labels = [label for _, label in self.dataset]\n",
    "        indexed_dataset = TensorDataset(torch.arange(len(self.dataset)), torch.tensor(train_labels))\n",
    "        accelerated_loader = DataLoader(indexed_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
    "\n",
    "        classifier.train()\n",
    "        for _ in range(config[\"EVAL_EPOCHS\"]):\n",
    "            for indices, labels in accelerated_loader:\n",
    "                features = cached_features[indices.to(config[\"DEVICE\"])]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = classifier(features)\n",
    "                loss = criterion(outputs, labels.to(config[\"DEVICE\"]))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        classifier.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for indices, labels in accelerated_loader:\n",
    "                features = cached_features[indices.to(config[\"DEVICE\"])]\n",
    "                outputs = classifier(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.to(config[\"DEVICE\"])).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        fitness = accuracy / (caching_time + 1)\n",
    "\n",
    "        print(f\" -> Cache Time: {caching_time:.2f}s | Accuracy: {accuracy:.2f}% | Fitness: {fitness:.2f}\")\n",
    "\n",
    "        del backbone, classifier, cached_features\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return {\"genome\": genome, \"accuracy\": accuracy, \"latency\": caching_time, \"fitness\": fitness}\n",
    "\n",
    "    def run_search(self):\n",
    "        print(\"--- Initializing Generation 0 ---\")\n",
    "        for _ in range(config[\"EVOLUTION_POPULATION_SIZE\"]):\n",
    "            self.population.append(self._generate_random_genome())\n",
    "\n",
    "        for gen in range(config[\"EVOLUTION_GENERATIONS\"]):\n",
    "            print(f\"\\n\\n{'='*20} EVOLUTION GENERATION {gen+1}/{config['EVOLUTION_GENERATIONS']} {'='*20}\")\n",
    "\n",
    "            evaluated_population = [self._evaluate_genome_with_caft(genome) for genome in self.population]\n",
    "            evaluated_population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "            self.history.append(evaluated_population)\n",
    "\n",
    "            print(f\"\\n--- Top Model of Generation {gen+1} ---\")\n",
    "            best_model = evaluated_population[0]\n",
    "            print(f\"  Genome: {' -> '.join(best_model['genome'])}\")\n",
    "            print(f\"  Accuracy: {best_model['accuracy']:.2f}% | Latency: {best_model['latency']:.2f}s | Fitness: {best_model['fitness']:.2f}\")\n",
    "\n",
    "            parents = [p['genome'] for p in evaluated_population[:config[\"EVOLUTION_NUM_PARENTS\"]]]\n",
    "            next_population = parents\n",
    "\n",
    "            while len(next_population) < config[\"EVOLUTION_POPULATION_SIZE\"]:\n",
    "                parent = random.choice(parents)\n",
    "                child = self._mutate_genome(parent)\n",
    "                next_population.append(child)\n",
    "\n",
    "            self.population = next_population\n",
    "\n",
    "        print(\"\\n\\n--- Search Complete ---\")\n",
    "        return self.history[-1][0]\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Building Module Library (this may take a moment) ---\")\n",
    "    resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "    module_library = {\n",
    "        'resnet_stem': {'module': nn.Sequential(resnet50.conv1, resnet50.bn1, resnet50.relu, resnet50.maxpool),\n",
    "                        'input_type': 'image', 'output_type': 'image', 'input_channels': 3, 'output_channels': 64},\n",
    "        'resnet_layer1': {'module': resnet50.layer1, 'input_type': 'image', 'output_type': 'image', 'input_channels': 64, 'output_channels': 256},\n",
    "        'resnet_layer2': {'module': resnet50.layer2, 'input_type': 'image', 'output_type': 'image', 'input_channels': 256, 'output_channels': 512},\n",
    "        'resnet_layer3': {'module': resnet50.layer3, 'input_type': 'image', 'output_type': 'image', 'input_channels': 512, 'output_channels': 1024},\n",
    "        'transformer_encoder': {'module': vit.encoder.layers[0], 'input_type': 'sequence', 'output_type': 'sequence', 'input_channels': 768, 'output_channels': 768},\n",
    "    }\n",
    "\n",
    "    for key in module_library:\n",
    "        for param in module_library[key]['module'].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # --- UPDATED SEARCH SPACE to demonstrate channel compatibility ---\n",
    "    search_space = [\n",
    "        ['resnet_stem'],\n",
    "        ['resnet_layer1'],\n",
    "        # Now, the search must discover that only resnet_layer2 can follow resnet_layer1\n",
    "        ['resnet_layer2'],\n",
    "        ['resnet_layer3'],\n",
    "        ['transformer_encoder'],\n",
    "    ]\n",
    "\n",
    "    # Example of a more complex search space you could use:\n",
    "    # search_space = [\n",
    "    #     ['resnet_stem'], # Stage 0\n",
    "    #     ['resnet_layer1'], # Stage 1\n",
    "    #     ['resnet_layer2'], # Stage 2\n",
    "    #     # Stage 3: A choice between going deeper with CNN or switching to Transformer\n",
    "    #     ['resnet_layer3', 'transformer_encoder']\n",
    "    # ]\n",
    "\n",
    "\n",
    "    print(\"\\n--- Loading Food101 Dataset ---\")\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    full_train_dataset = torchvision.datasets.Food101(root=config[\"DATA_DIR\"], split='train', download=False, transform=data_transforms)\n",
    "    subset_indices = random.sample(range(len(full_train_dataset)), 5000)\n",
    "    train_dataset_subset = torch.utils.data.Subset(full_train_dataset, subset_indices)\n",
    "    print(f\"Using a subset of {len(train_dataset_subset)} images for the search.\")\n",
    "\n",
    "    nas = NeuralArchitectureSearch(search_space, module_library, train_dataset_subset)\n",
    "    best_discovered_model = nas.run_search()\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"      BEST DISCOVERED ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Genome: {' -> '.join(best_discovered_model['genome'])}\")\n",
    "    print(f\"  Final Accuracy: {best_discovered_model['accuracy']:.2f}%\")\n",
    "    print(f\"  Backbone Latency (Caching Time): {best_discovered_model['latency']:.2f}s\")\n",
    "    print(f\"  Final Fitness Score: {best_discovered_model['fitness']:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Module Library (this may take a moment) ---\n",
      "\n",
      "--- Loading Food101 Dataset ---\n",
      "Using a subset of 5000 images for the search.\n",
      "--- Initializing Generation 0 ---\n",
      "\n",
      "\n",
      "==================== EVOLUTION GENERATION 1/5 ====================\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> resnet_layer3 -> transformer_encoder\n",
      " -> Cache Time: 13.96s | Accuracy: 8.48% | Fitness: 0.57\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> resnet_layer3 -> transformer_encoder\n",
      " -> Cache Time: 12.96s | Accuracy: 13.02% | Fitness: 0.93\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> resnet_layer3 -> transformer_encoder\n",
      " -> Cache Time: 14.02s | Accuracy: 12.56% | Fitness: 0.84\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> resnet_layer3 -> transformer_encoder\n",
      " -> Cache Time: 13.11s | Accuracy: 13.44% | Fitness: 0.95\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> resnet_layer3 -> transformer_encoder\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 287\u001B[39m\n\u001B[32m    284\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing a subset of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_dataset_subset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m images for the search.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    286\u001B[39m nas = NeuralArchitectureSearch(search_space, module_library, train_dataset_subset)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m best_discovered_model = \u001B[43mnas\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    289\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m60\u001B[39m)\n\u001B[32m    290\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m      BEST DISCOVERED ARCHITECTURE\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 214\u001B[39m, in \u001B[36mNeuralArchitectureSearch.run_search\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    211\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m gen \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config[\u001B[33m\"\u001B[39m\u001B[33mEVOLUTION_GENERATIONS\u001B[39m\u001B[33m\"\u001B[39m]):\n\u001B[32m    212\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m20\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m EVOLUTION GENERATION \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgen+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[33m'\u001B[39m\u001B[33mEVOLUTION_GENERATIONS\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m20\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m214\u001B[39m     evaluated_population = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_evaluate_genome_with_caft\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenome\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m genome \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.population]\n\u001B[32m    215\u001B[39m     evaluated_population.sort(key=\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[33m'\u001B[39m\u001B[33mfitness\u001B[39m\u001B[33m'\u001B[39m], reverse=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    216\u001B[39m     \u001B[38;5;28mself\u001B[39m.history.append(evaluated_population)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 171\u001B[39m, in \u001B[36mNeuralArchitectureSearch._evaluate_genome_with_caft\u001B[39m\u001B[34m(self, genome)\u001B[39m\n\u001B[32m    168\u001B[39m caching_time = time.time() - caching_start_time\n\u001B[32m    169\u001B[39m cached_features = cached_features.to(config[\u001B[33m\"\u001B[39m\u001B[33mDEVICE\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m train_labels = \u001B[43m[\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m    172\u001B[39m indexed_dataset = TensorDataset(torch.arange(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset)), torch.tensor(train_labels))\n\u001B[32m    173\u001B[39m accelerated_loader = DataLoader(indexed_dataset, batch_size=config[\u001B[33m\"\u001B[39m\u001B[33mBATCH_SIZE\u001B[39m\u001B[33m\"\u001B[39m], shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/utils/data/dataset.py:408\u001B[39m, in \u001B[36mSubset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[32m    407\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset[[\u001B[38;5;28mself\u001B[39m.indices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[32m--> \u001B[39m\u001B[32m408\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torchvision/datasets/food101.py:82\u001B[39m, in \u001B[36mFood101.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     79\u001B[39m image = \u001B[38;5;28mself\u001B[39m.loader(image_file)\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform:\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     image = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     84\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.target_transform:\n\u001B[32m     85\u001B[39m     label = \u001B[38;5;28mself\u001B[39m.target_transform(label)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torchvision/transforms/transforms.py:354\u001B[39m, in \u001B[36mResize.forward\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m    346\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m    347\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    348\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    349\u001B[39m \u001B[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    352\u001B[39m \u001B[33;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001B[39m, in \u001B[36mresize\u001B[39m\u001B[34m(img, size, interpolation, max_size, antialias)\u001B[39m\n\u001B[32m    475\u001B[39m         warnings.warn(\u001B[33m\"\u001B[39m\u001B[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    476\u001B[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001B[32m--> \u001B[39m\u001B[32m477\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpil_interpolation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    479\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:250\u001B[39m, in \u001B[36mresize\u001B[39m\u001B[34m(img, size, interpolation)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) == \u001B[32m2\u001B[39m):\n\u001B[32m    248\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGot inappropriate size arg: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/PIL/Image.py:2365\u001B[39m, in \u001B[36mImage.resize\u001B[39m\u001B[34m(self, size, resample, box, reducing_gap)\u001B[39m\n\u001B[32m   2353\u001B[39m         \u001B[38;5;28mself\u001B[39m = (\n\u001B[32m   2354\u001B[39m             \u001B[38;5;28mself\u001B[39m.reduce(factor, box=reduce_box)\n\u001B[32m   2355\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m.reduce)\n\u001B[32m   2356\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m Image.reduce(\u001B[38;5;28mself\u001B[39m, factor, box=reduce_box)\n\u001B[32m   2357\u001B[39m         )\n\u001B[32m   2358\u001B[39m         box = (\n\u001B[32m   2359\u001B[39m             (box[\u001B[32m0\u001B[39m] - reduce_box[\u001B[32m0\u001B[39m]) / factor_x,\n\u001B[32m   2360\u001B[39m             (box[\u001B[32m1\u001B[39m] - reduce_box[\u001B[32m1\u001B[39m]) / factor_y,\n\u001B[32m   2361\u001B[39m             (box[\u001B[32m2\u001B[39m] - reduce_box[\u001B[32m0\u001B[39m]) / factor_x,\n\u001B[32m   2362\u001B[39m             (box[\u001B[32m3\u001B[39m] - reduce_box[\u001B[32m1\u001B[39m]) / factor_y,\n\u001B[32m   2363\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m2365\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mim\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbox\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d9acddd7f4e7c969",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
