{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T18:50:10.354694Z",
     "start_time": "2025-08-29T18:48:47.623793Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# --- 1. Global Configuration ---\n",
    "config = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"DATA_DIR\": \"/home/kami/Documents/datasets/\",\n",
    "    \"NUM_CLASSES\": 101,\n",
    "    \"EVOLUTION_POPULATION_SIZE\": 10,\n",
    "    \"EVOLUTION_GENERATIONS\": 5,\n",
    "    \"EVOLUTION_NUM_PARENTS\": 3,\n",
    "    \"EVAL_EPOCHS\": 1,\n",
    "}\n",
    "\n",
    "# --- 2. Modular AI Components ---\n",
    "\n",
    "# ================================================================= #\n",
    "# --- CORRECTED HELPER MODULES ---\n",
    "class Permute(nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super().__init__() # CRITICAL: Call parent constructor\n",
    "        self.dims = dims\n",
    "    def forward(self, x):\n",
    "        return x.permute(*self.dims)\n",
    "\n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__() # CRITICAL: Call parent constructor\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(self.dim)\n",
    "# ================================================================= #\n",
    "\n",
    "\n",
    "class CNNToTransformerAdapter(nn.Module):\n",
    "    def __init__(self, cnn_out_channels, transformer_embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(cnn_out_channels, transformer_embed_dim, kernel_size=1)\n",
    "        self.flatten = nn.Flatten(2)\n",
    "        self.norm = nn.LayerNorm(transformer_embed_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x); x = self.flatten(x); x = x.permute(0, 2, 1); x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class AssembledBackbone(nn.Module):\n",
    "    def __init__(self, genome, module_library):\n",
    "        super().__init__()\n",
    "        self.genome = genome\n",
    "        self.module_list = nn.ModuleList()\n",
    "        self.output_channels = 0\n",
    "\n",
    "        current_channels = 3\n",
    "        current_type = 'image'\n",
    "\n",
    "        for module_name in genome:\n",
    "            module_info = module_library[module_name]\n",
    "            if module_info['input_type'] == 'sequence' and current_type == 'image':\n",
    "                adapter = CNNToTransformerAdapter(current_channels, module_info['input_channels'])\n",
    "                self.module_list.append(adapter)\n",
    "\n",
    "            module_instance = copy.deepcopy(module_info['module'])\n",
    "            self.module_list.append(module_instance)\n",
    "            current_channels = module_info['output_channels']\n",
    "            current_type = module_info['output_type']\n",
    "\n",
    "        if current_type == 'image':\n",
    "            self.module_list.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            self.module_list.append(nn.Flatten(1))\n",
    "            self.output_channels = current_channels\n",
    "        else:\n",
    "            self.module_list.append(Permute(0, 2, 1))\n",
    "            self.module_list.append(nn.AdaptiveAvgPool1d(1))\n",
    "            self.module_list.append(Squeeze(-1))\n",
    "            self.output_channels = current_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# --- 3. The Search System (Evolutionary Algorithm) ---\n",
    "class NeuralArchitectureSearch:\n",
    "    def __init__(self, search_space, module_library, dataset):\n",
    "        self.search_space = search_space\n",
    "        self.module_library = module_library\n",
    "        self.dataset = dataset\n",
    "        self.population = []\n",
    "        self.history = []\n",
    "\n",
    "    def _generate_random_genome(self):\n",
    "        genome = []\n",
    "        current_type = 'image'\n",
    "        for stage in self.search_space:\n",
    "            possible_modules = [\n",
    "                m for m in stage if\n",
    "                (self.module_library[m]['input_type'] == current_type) or\n",
    "                (self.module_library[m]['input_type'] == 'sequence' and current_type == 'image')\n",
    "            ]\n",
    "            chosen_module = random.choice(possible_modules)\n",
    "            genome.append(chosen_module)\n",
    "            current_type = self.module_library[chosen_module]['output_type']\n",
    "        return genome\n",
    "\n",
    "    def _mutate_genome(self, genome):\n",
    "        mutation_point = random.randint(0, len(genome) - 1)\n",
    "        if mutation_point == 0: input_type_needed = 'image'\n",
    "        else: input_type_needed = self.module_library[genome[mutation_point - 1]]['output_type']\n",
    "        stage = self.search_space[mutation_point]\n",
    "        compatible_replacements = [\n",
    "            m for m in stage if\n",
    "            (self.module_library[m]['input_type'] == input_type_needed) or\n",
    "            (self.module_library[m]['input_type'] == 'sequence' and input_type_needed == 'image')\n",
    "        ]\n",
    "        new_module_name = random.choice(compatible_replacements)\n",
    "        new_genome = copy.deepcopy(genome)\n",
    "        new_genome[mutation_point] = new_module_name\n",
    "        return new_genome\n",
    "\n",
    "    def _evaluate_genome_with_caft(self, genome):\n",
    "        print(f\"\\nEvaluating Genome: {' -> '.join(genome)}\")\n",
    "\n",
    "        backbone = AssembledBackbone(genome, self.module_library).to(config[\"DEVICE\"])\n",
    "        for param in backbone.parameters(): param.requires_grad = False\n",
    "        backbone.eval()\n",
    "\n",
    "        output_dim = backbone.output_channels\n",
    "        classifier = nn.Linear(output_dim, config[\"NUM_CLASSES\"]).to(config[\"DEVICE\"])\n",
    "        optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        caching_start_time = time.time()\n",
    "        caching_loader = DataLoader(self.dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)\n",
    "        cached_features = torch.zeros(len(self.dataset), output_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            start_idx = 0\n",
    "            for images, _ in caching_loader:\n",
    "                features = backbone(images.to(config[\"DEVICE\"]))\n",
    "                cached_features[start_idx : start_idx + images.size(0)] = features.cpu()\n",
    "                start_idx += images.size(0)\n",
    "        caching_time = time.time() - caching_start_time\n",
    "        cached_features = cached_features.to(config[\"DEVICE\"])\n",
    "\n",
    "        train_labels = [label for _, label in self.dataset]\n",
    "        indexed_dataset = TensorDataset(torch.arange(len(self.dataset)), torch.tensor(train_labels))\n",
    "        accelerated_loader = DataLoader(indexed_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
    "\n",
    "        classifier.train()\n",
    "        for _ in range(config[\"EVAL_EPOCHS\"]):\n",
    "            for indices, labels in accelerated_loader:\n",
    "                features = cached_features[indices.to(config[\"DEVICE\"])]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = classifier(features)\n",
    "                loss = criterion(outputs, labels.to(config[\"DEVICE\"]))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        classifier.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for indices, labels in accelerated_loader:\n",
    "                features = cached_features[indices.to(config[\"DEVICE\"])]\n",
    "                outputs = classifier(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.to(config[\"DEVICE\"])).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        fitness = accuracy / (caching_time + 1)\n",
    "\n",
    "        print(f\" -> Cache Time: {caching_time:.2f}s | Accuracy: {accuracy:.2f}% | Fitness: {fitness:.2f}\")\n",
    "\n",
    "        del backbone, classifier, cached_features\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return {\"genome\": genome, \"accuracy\": accuracy, \"latency\": caching_time, \"fitness\": fitness}\n",
    "\n",
    "    def run_search(self):\n",
    "        print(\"--- Initializing Generation 0 ---\")\n",
    "        for _ in range(config[\"EVOLUTION_POPULATION_SIZE\"]):\n",
    "            self.population.append(self._generate_random_genome())\n",
    "\n",
    "        for gen in range(config[\"EVOLUTION_GENERATIONS\"]):\n",
    "            print(f\"\\n\\n{'='*20} EVOLUTION GENERATION {gen+1}/{config['EVOLUTION_GENERATIONS']} {'='*20}\")\n",
    "\n",
    "            evaluated_population = [self._evaluate_genome_with_caft(genome) for genome in self.population]\n",
    "            evaluated_population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "            self.history.append(evaluated_population)\n",
    "\n",
    "            print(f\"\\n--- Top Model of Generation {gen+1} ---\")\n",
    "            best_model = evaluated_population[0]\n",
    "            print(f\"  Genome: {' -> '.join(best_model['genome'])}\")\n",
    "            print(f\"  Accuracy: {best_model['accuracy']:.2f}% | Latency: {best_model['latency']:.2f}s | Fitness: {best_model['fitness']:.2f}\")\n",
    "\n",
    "            parents = [p['genome'] for p in evaluated_population[:config[\"EVOLUTION_NUM_PARENTS\"]]]\n",
    "            next_population = parents\n",
    "\n",
    "            while len(next_population) < config[\"EVOLUTION_POPULATION_SIZE\"]:\n",
    "                parent = random.choice(parents)\n",
    "                child = self._mutate_genome(parent)\n",
    "                next_population.append(child)\n",
    "\n",
    "            self.population = next_population\n",
    "\n",
    "        print(\"\\n\\n--- Search Complete ---\")\n",
    "        return self.history[-1][0]\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Building Module Library (this may take a moment) ---\")\n",
    "    resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "    module_library = {\n",
    "        'resnet_stem': {'module': nn.Sequential(resnet50.conv1, resnet50.bn1, resnet50.relu, resnet50.maxpool),\n",
    "                        'input_type': 'image', 'output_type': 'image', 'input_channels': 3, 'output_channels': 64},\n",
    "        'resnet_layer1': {'module': resnet50.layer1, 'input_type': 'image', 'output_type': 'image', 'input_channels': 64, 'output_channels': 256},\n",
    "        'resnet_layer2': {'module': resnet50.layer2, 'input_type': 'image', 'output_type': 'image', 'input_channels': 256, 'output_channels': 512},\n",
    "        'resnet_layer3': {'module': resnet50.layer3, 'input_type': 'image', 'output_type': 'image', 'input_channels': 512, 'output_channels': 1024},\n",
    "        'transformer_encoder': {'module': vit.encoder.layers[0], 'input_type': 'sequence', 'output_type': 'sequence', 'input_channels': 768, 'output_channels': 768},\n",
    "    }\n",
    "\n",
    "    for key in module_library:\n",
    "        for param in module_library[key]['module'].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    search_space = [\n",
    "        ['resnet_stem'],\n",
    "        ['resnet_layer1'],\n",
    "        ['resnet_layer2', 'resnet_layer3'],\n",
    "        ['transformer_encoder'],\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Loading Food101 Dataset ---\")\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    full_train_dataset = torchvision.datasets.Food101(root=config[\"DATA_DIR\"], split='train', download=False, transform=data_transforms)\n",
    "    subset_indices = random.sample(range(len(full_train_dataset)), 5000)\n",
    "    train_dataset_subset = torch.utils.data.Subset(full_train_dataset, subset_indices)\n",
    "    print(f\"Using a subset of {len(train_dataset_subset)} images for the search.\")\n",
    "\n",
    "    nas = NeuralArchitectureSearch(search_space, module_library, train_dataset_subset)\n",
    "    best_discovered_model = nas.run_search()\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"      BEST DISCOVERED ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Genome: {' -> '.join(best_discovered_model['genome'])}\")\n",
    "    print(f\"  Final Accuracy: {best_discovered_model['accuracy']:.2f}%\")\n",
    "    print(f\"  Backbone Latency (Caching Time): {best_discovered_model['latency']:.2f}s\")\n",
    "    print(f\"  Final Fitness Score: {best_discovered_model['fitness']:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Module Library (this may take a moment) ---\n",
      "\n",
      "--- Loading Food101 Dataset ---\n",
      "Using a subset of 5000 images for the search.\n",
      "--- Initializing Generation 0 ---\n",
      "\n",
      "\n",
      "==================== EVOLUTION GENERATION 1/5 ====================\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> transformer_encoder\n",
      " -> Cache Time: 16.34s | Accuracy: 5.28% | Fitness: 0.30\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> transformer_encoder\n",
      " -> Cache Time: 16.76s | Accuracy: 6.36% | Fitness: 0.36\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer2 -> transformer_encoder\n",
      " -> Cache Time: 15.94s | Accuracy: 3.96% | Fitness: 0.23\n",
      "\n",
      "Evaluating Genome: resnet_stem -> resnet_layer1 -> resnet_layer3 -> transformer_encoder\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 512, 1, 1], expected input[128, 256, 56, 56] to have 512 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 259\u001B[39m\n\u001B[32m    256\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing a subset of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_dataset_subset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m images for the search.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    258\u001B[39m nas = NeuralArchitectureSearch(search_space, module_library, train_dataset_subset)\n\u001B[32m--> \u001B[39m\u001B[32m259\u001B[39m best_discovered_model = \u001B[43mnas\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m60\u001B[39m)\n\u001B[32m    262\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m      BEST DISCOVERED ARCHITECTURE\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 199\u001B[39m, in \u001B[36mNeuralArchitectureSearch.run_search\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    196\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m gen \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config[\u001B[33m\"\u001B[39m\u001B[33mEVOLUTION_GENERATIONS\u001B[39m\u001B[33m\"\u001B[39m]):\n\u001B[32m    197\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m20\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m EVOLUTION GENERATION \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgen+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[33m'\u001B[39m\u001B[33mEVOLUTION_GENERATIONS\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m20\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m     evaluated_population = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_evaluate_genome_with_caft\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenome\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m genome \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.population]\n\u001B[32m    200\u001B[39m     evaluated_population.sort(key=\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[33m'\u001B[39m\u001B[33mfitness\u001B[39m\u001B[33m'\u001B[39m], reverse=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    201\u001B[39m     \u001B[38;5;28mself\u001B[39m.history.append(evaluated_population)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 150\u001B[39m, in \u001B[36mNeuralArchitectureSearch._evaluate_genome_with_caft\u001B[39m\u001B[34m(self, genome)\u001B[39m\n\u001B[32m    148\u001B[39m start_idx = \u001B[32m0\u001B[39m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m images, _ \u001B[38;5;129;01min\u001B[39;00m caching_loader:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m     features = \u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mDEVICE\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m     cached_features[start_idx : start_idx + images.size(\u001B[32m0\u001B[39m)] = features.cpu()\n\u001B[32m    152\u001B[39m     start_idx += images.size(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 90\u001B[39m, in \u001B[36mAssembledBackbone.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     89\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.module_list:\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m         x = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     91\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torchvision/models/resnet.py:146\u001B[39m, in \u001B[36mBottleneck.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    143\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) -> Tensor:\n\u001B[32m    144\u001B[39m     identity = x\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m     out = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    147\u001B[39m     out = \u001B[38;5;28mself\u001B[39m.bn1(out)\n\u001B[32m    148\u001B[39m     out = \u001B[38;5;28mself\u001B[39m.relu(out)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/cuda312/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Given groups=1, weight of size [256, 512, 1, 1], expected input[128, 256, 56, 56] to have 512 channels, but got 256 channels instead"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d9acddd7f4e7c969"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
