{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T23:47:08.426387Z",
     "start_time": "2025-08-10T23:36:44.065346Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "config = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"NUM_EPOCHS\": 10,\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"DATA_DIR\": \"/home/kami/Documents/datasets/\",\n",
    "    \"NUM_CLASSES\": 101,\n",
    "    \"VGG16_FEATURE_SIZE\": 25088\n",
    "}\n",
    "\n",
    "print(f\"--- SCRIPT 1: CACHED FINE-TUNING ---\")\n",
    "print(f\"Using device: {config['DEVICE']}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 2. The Custom Caching Model ---\n",
    "class CachedFineTuneModel(nn.Module):\n",
    "    def __init__(self, original_model, num_classes, num_records):\n",
    "        super().__init__()\n",
    "        self.features = original_model.features\n",
    "        self.avgpool = original_model.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config[\"VGG16_FEATURE_SIZE\"], 4096), nn.ReLU(True), nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 1024), nn.ReLU(True), nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        initial_cache = torch.zeros(num_records, config[\"VGG16_FEATURE_SIZE\"])\n",
    "        self.register_buffer('frozen_data', initial_cache)\n",
    "        self.register_buffer('is_cached', torch.tensor(False))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def cache_activations(self, dataloader: DataLoader):\n",
    "        print(\"--- Phase 1: Caching Activations (One-Time Cost) ---\")\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        start_time = time.time()\n",
    "        for i, (data, _) in enumerate(tqdm(dataloader, desc=\"Caching Progress\")):\n",
    "            data = data.to(device)\n",
    "            batch_size = data.shape[0]\n",
    "            start_index, end_index = i * dataloader.batch_size, i * dataloader.batch_size + batch_size\n",
    "            activations = torch.flatten(self.avgpool(self.features(data)), 1)\n",
    "            self.frozen_data[start_index:end_index] = activations.cpu()\n",
    "        self.is_cached.fill_(True)\n",
    "        print(f\"Caching complete in {time.time() - start_time:.2f} seconds.\")\n",
    "        self.frozen_data = self.frozen_data.to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training and self.is_cached:\n",
    "            return self.classifier(self.frozen_data[x])\n",
    "        else: # Used for a single forward pass if needed, not in training loop\n",
    "            return self.classifier(torch.flatten(self.avgpool(self.features(x)), 1))\n",
    "\n",
    "# --- 3. Data and Model Setup ---\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "print(\"Loading Food101 dataset for caching...\")\n",
    "train_dataset_caching = datasets.Food101(root=config[\"DATA_DIR\"], split='train', download=True, transform=data_transforms)\n",
    "caching_loader = DataLoader(train_dataset_caching, batch_size=config[\"BATCH_SIZE\"], shuffle=False)\n",
    "vgg16_bn = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "for param in vgg16_bn.features.parameters(): param.requires_grad = False\n",
    "model = CachedFineTuneModel(vgg16_bn, config[\"NUM_CLASSES\"], len(train_dataset_caching)).to(config[\"DEVICE\"])\n",
    "\n",
    "# --- 4. Caching Phase ---\n",
    "model.cache_activations(caching_loader)\n",
    "\n",
    "# --- 5. Accelerated Training Phase ---\n",
    "print(\"\\n--- Phase 2: Accelerated Fine-Tuning ---\")\n",
    "train_labels = [label for _, label in train_dataset_caching]\n",
    "training_dataset_indexed = TensorDataset(torch.arange(len(train_dataset_caching)), torch.tensor(train_labels))\n",
    "training_loader = DataLoader(training_dataset_indexed, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=config[\"LEARNING_RATE\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_training_time = 0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(config[\"NUM_EPOCHS\"]):\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(training_loader, desc=f\"Epoch {epoch+1}/{config['NUM_EPOCHS']}\")\n",
    "    for batch_indices, batch_labels in pbar:\n",
    "        batch_indices, batch_labels = batch_indices.to(config[\"DEVICE\"]), batch_labels.to(config[\"DEVICE\"])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_indices)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_training_time += epoch_time\n",
    "\n",
    "# --- 6. Final Report ---\n",
    "final_train_loss = running_loss / len(training_loader)\n",
    "final_train_accuracy = 100 * correct / total\n",
    "print(\"-\" * 40)\n",
    "print(f\"Finished Accelerated Training.\")\n",
    "print(f\"Total Time for {config['NUM_EPOCHS']} epochs: {total_training_time:.2f} seconds\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Training Accuracy: {final_train_accuracy:.2f}%\")\n",
    "print(\"-\" * 40)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCRIPT 1: CACHED FINE-TUNING ---\n",
      "Using device: cuda\n",
      "----------------------------------------\n",
      "Loading Food101 dataset for caching...\n",
      "--- Phase 1: Caching Activations (One-Time Cost) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching Progress: 100%|██████████| 1184/1184 [04:36<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching complete in 276.60 seconds.\n",
      "\n",
      "--- Phase 2: Accelerated Fine-Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1184/1184 [00:15<00:00, 77.06it/s, loss=2.5550]\n",
      "Epoch 2/10: 100%|██████████| 1184/1184 [00:15<00:00, 78.36it/s, loss=2.7821]\n",
      "Epoch 3/10: 100%|██████████| 1184/1184 [00:15<00:00, 78.18it/s, loss=1.9598]\n",
      "Epoch 4/10: 100%|██████████| 1184/1184 [00:14<00:00, 78.98it/s, loss=2.1403]\n",
      "Epoch 5/10: 100%|██████████| 1184/1184 [00:14<00:00, 82.42it/s, loss=1.5186]\n",
      "Epoch 6/10: 100%|██████████| 1184/1184 [00:15<00:00, 77.66it/s, loss=2.2002]\n",
      "Epoch 7/10: 100%|██████████| 1184/1184 [00:14<00:00, 80.10it/s, loss=1.3070]\n",
      "Epoch 8/10: 100%|██████████| 1184/1184 [00:14<00:00, 79.67it/s, loss=1.6209]\n",
      "Epoch 9/10: 100%|██████████| 1184/1184 [00:14<00:00, 81.40it/s, loss=2.0822]\n",
      "Epoch 10/10: 100%|██████████| 1184/1184 [00:14<00:00, 80.27it/s, loss=2.5154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Finished Accelerated Training.\n",
      "Total Time for 10 epochs: 149.17 seconds\n",
      "Final Training Loss: 1.6591\n",
      "Final Training Accuracy: 59.02%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b638789904cf6209"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
