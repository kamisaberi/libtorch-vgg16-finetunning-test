{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T17:15:51.522535Z",
     "start_time": "2025-08-29T17:15:49.751054Z"
    }
   },
   "source": [
    "# create_module_library.py\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "\n",
    "print(\"Creating the pre-trained submodule library...\")\n",
    "os.makedirs(\"module_library\", exist_ok=True)\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet_stem = torch.nn.Sequential(resnet50.conv1, resnet50.bn1, resnet50.relu, resnet50.maxpool)\n",
    "torch.save(resnet_stem.state_dict(), \"module_library/resnet_stem.pth\")\n",
    "torch.save(resnet50.layer1.state_dict(), \"module_library/resnet_layer1.pth\")\n",
    "torch.save(resnet50.layer2.state_dict(), \"module_library/resnet_layer2.pth\")\n",
    "vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "transformer_block = vit.encoder.layers[0]\n",
    "torch.save(transformer_block.state_dict(), \"module_library/transformer_encoder_block.pth\")\n",
    "print(\"Module library created successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the pre-trained submodule library...\n",
      "Module library created successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:29:06.727436Z",
     "start_time": "2025-08-29T17:24:09.577128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# main_modular_caft.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# --- 1. Global Configuration ---\n",
    "config = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"NUM_EPOCHS\": 100,\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"DATA_DIR\": \"/home/kami/Documents/datasets/\",\n",
    "    \"NUM_CLASSES\": 101,\n",
    "}\n",
    "\n",
    "# --- 2. Adapter Module (The \"Glue\") ---\n",
    "class CNNToTransformerAdapter(nn.Module):\n",
    "    def __init__(self, cnn_out_channels, transformer_embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(cnn_out_channels, transformer_embed_dim, kernel_size=1)\n",
    "        self.flatten = nn.Flatten(2)\n",
    "        self.norm = nn.LayerNorm(transformer_embed_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x); x = self.flatten(x); x = x.permute(0, 2, 1); x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# --- 3. The Custom Modular Backbone to be Cached ---\n",
    "class ModularBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Assembling new ModularBackbone for caching...\")\n",
    "        # Load pre-trained CNN parts\n",
    "        resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.stem = torch.nn.Sequential(resnet50.conv1, resnet50.bn1, resnet50.relu, resnet50.maxpool)\n",
    "        self.stem.load_state_dict(torch.load(\"module_library/resnet_stem.pth\"))\n",
    "        self.layer1 = resnet50.layer1\n",
    "        self.layer1.load_state_dict(torch.load(\"module_library/resnet_layer1.pth\"))\n",
    "        self.layer2 = resnet50.layer2\n",
    "        self.layer2.load_state_dict(torch.load(\"module_library/resnet_layer2.pth\"))\n",
    "        # Load pre-trained Transformer part\n",
    "        vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "        self.transformer = vit.encoder.layers[0]\n",
    "        self.transformer.load_state_dict(torch.load(\"module_library/transformer_encoder_block.pth\"))\n",
    "        # Instantiate adapter\n",
    "        self.adapter = CNNToTransformerAdapter(cnn_out_channels=512, transformer_embed_dim=768)\n",
    "        # Final pooling layer to create a single feature vector per image\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The full flow of our custom feature extractor\n",
    "        x = self.stem(x)    # CNN part\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.adapter(x) # Glue\n",
    "        x = self.transformer(x) # Transformer part\n",
    "        # Pool the sequence of features into a single vector\n",
    "        x = x.permute(0, 2, 1) # [B, L, D] -> [B, D, L] for pooling\n",
    "        x = self.pool(x).squeeze(2) # [B, D, 1] -> [B, D]\n",
    "        return x\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using device: {config['DEVICE']}\")\n",
    "    # --- Shared Data Loading ---\n",
    "    print(\"Loading Food101 dataset...\")\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.Food101(root=config[\"DATA_DIR\"], split='train', download=False, transform=data_transforms)\n",
    "    num_train_records = len(train_dataset)\n",
    "\n",
    "    # ========================================================================\n",
    "    # --- MODULAR CAFT EXPERIMENT ---\n",
    "    # ========================================================================\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"        EXPERIMENT: CAFT with a Custom Modular Backbone\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Instantiate the custom backbone and the separate trainable classifier\n",
    "    modular_backbone = ModularBackbone().to(config['DEVICE'])\n",
    "    for param in modular_backbone.parameters(): # Freeze the entire custom backbone\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # The classifier head is small and trainable\n",
    "    # It takes the output of our backbone (768 dims)\n",
    "    classifier_head = nn.Linear(768, config[\"NUM_CLASSES\"]).to(config[\"DEVICE\"])\n",
    "\n",
    "    # 2. Phase 1: Caching with the Modular Backbone\n",
    "    caching_start_time = time.time()\n",
    "    print(\"\\n--- Phase 1: Caching Activations from Modular Backbone ---\")\n",
    "\n",
    "    caching_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False, num_workers=4)\n",
    "    cached_features = torch.zeros(num_train_records, 768) # Cache size matches backbone output\n",
    "\n",
    "    modular_backbone.eval()\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for images, _ in tqdm(caching_loader, desc=\"Caching Progress\"):\n",
    "            images = images.to(config[\"DEVICE\"])\n",
    "            # Get features from our custom backbone\n",
    "            features = modular_backbone(images)\n",
    "            cached_features[start_idx : start_idx + images.size(0)] = features.cpu()\n",
    "            start_idx += images.size(0)\n",
    "\n",
    "    caching_time = time.time() - caching_start_time\n",
    "    cached_features = cached_features.to(config[\"DEVICE\"])\n",
    "\n",
    "    # 3. Phase 2: Accelerated Training\n",
    "    print(\"\\n--- Phase 2: Accelerated Training of Classifier Head ---\")\n",
    "    train_labels = [label for _, label in train_dataset]\n",
    "    indexed_dataset = TensorDataset(torch.arange(num_train_records), torch.tensor(train_labels))\n",
    "    accelerated_loader = DataLoader(indexed_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(classifier_head.parameters(), lr=config[\"LEARNING_RATE\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    classifier_head.train()\n",
    "    for epoch in range(config[\"NUM_EPOCHS\"]):\n",
    "        pbar = tqdm(accelerated_loader, desc=f\"Modular CAFT Epoch {epoch+1}/{config['NUM_EPOCHS']}\")\n",
    "        correct, total = 0, 0\n",
    "        for indices, labels in pbar:\n",
    "            indices, labels = indices.to(config[\"DEVICE\"]), labels.to(config[\"DEVICE\"])\n",
    "            # Get features directly from the cache\n",
    "            features = cached_features[indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier_head(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch == config[\"NUM_EPOCHS\"] - 1: # Accumulate stats on the final epoch\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "    final_accuracy = 100 * correct / total\n",
    "\n",
    "    # 4. Final Report\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"--- Modular CAFT Results ---\")\n",
    "    print(f\"Backbone: Custom Hybrid (ResNet + Adapter + Transformer)\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Caching Phase Time:           {caching_time:.2f} seconds\")\n",
    "    print(f\"Accelerated Training Time:    {training_time:.2f} seconds (for {config['NUM_EPOCHS']} epochs)\")\n",
    "    print(f\"Total Time Elapsed:           {caching_time + training_time:.2f} seconds\")\n",
    "    print(f\"Final Training Accuracy:      {final_accuracy:.2f}%\")\n",
    "    print(\"=\"*60)"
   ],
   "id": "3281ae5a2dbd0718",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Food101 dataset...\n",
      "\n",
      "\n",
      "============================================================\n",
      "        EXPERIMENT: CAFT with a Custom Modular Backbone\n",
      "============================================================\n",
      "Assembling new ModularBackbone for caching...\n",
      "\n",
      "--- Phase 1: Caching Activations from Modular Backbone ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching Progress: 100%|██████████| 1184/1184 [01:49<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Accelerated Training of Classifier Head ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modular CAFT Epoch 1/100: 100%|██████████| 1184/1184 [00:00<00:00, 2432.90it/s]\n",
      "Modular CAFT Epoch 2/100: 100%|██████████| 1184/1184 [00:00<00:00, 2469.62it/s]\n",
      "Modular CAFT Epoch 3/100: 100%|██████████| 1184/1184 [00:00<00:00, 2315.08it/s]\n",
      "Modular CAFT Epoch 4/100: 100%|██████████| 1184/1184 [00:00<00:00, 2443.44it/s]\n",
      "Modular CAFT Epoch 5/100: 100%|██████████| 1184/1184 [00:00<00:00, 2210.98it/s]\n",
      "Modular CAFT Epoch 6/100: 100%|██████████| 1184/1184 [00:00<00:00, 2447.76it/s]\n",
      "Modular CAFT Epoch 7/100: 100%|██████████| 1184/1184 [00:00<00:00, 2375.70it/s]\n",
      "Modular CAFT Epoch 8/100: 100%|██████████| 1184/1184 [00:00<00:00, 2313.89it/s]\n",
      "Modular CAFT Epoch 9/100: 100%|██████████| 1184/1184 [00:00<00:00, 2137.87it/s]\n",
      "Modular CAFT Epoch 10/100: 100%|██████████| 1184/1184 [00:00<00:00, 2439.09it/s]\n",
      "Modular CAFT Epoch 11/100: 100%|██████████| 1184/1184 [00:00<00:00, 2309.99it/s]\n",
      "Modular CAFT Epoch 12/100: 100%|██████████| 1184/1184 [00:00<00:00, 2374.87it/s]\n",
      "Modular CAFT Epoch 13/100: 100%|██████████| 1184/1184 [00:00<00:00, 2467.92it/s]\n",
      "Modular CAFT Epoch 14/100: 100%|██████████| 1184/1184 [00:00<00:00, 2188.52it/s]\n",
      "Modular CAFT Epoch 15/100: 100%|██████████| 1184/1184 [00:00<00:00, 2385.12it/s]\n",
      "Modular CAFT Epoch 16/100: 100%|██████████| 1184/1184 [00:00<00:00, 2211.42it/s]\n",
      "Modular CAFT Epoch 17/100: 100%|██████████| 1184/1184 [00:00<00:00, 2277.06it/s]\n",
      "Modular CAFT Epoch 18/100: 100%|██████████| 1184/1184 [00:00<00:00, 2426.12it/s]\n",
      "Modular CAFT Epoch 19/100: 100%|██████████| 1184/1184 [00:00<00:00, 2497.68it/s]\n",
      "Modular CAFT Epoch 20/100: 100%|██████████| 1184/1184 [00:00<00:00, 2249.68it/s]\n",
      "Modular CAFT Epoch 21/100: 100%|██████████| 1184/1184 [00:00<00:00, 2406.48it/s]\n",
      "Modular CAFT Epoch 22/100: 100%|██████████| 1184/1184 [00:00<00:00, 2236.07it/s]\n",
      "Modular CAFT Epoch 23/100: 100%|██████████| 1184/1184 [00:00<00:00, 2325.79it/s]\n",
      "Modular CAFT Epoch 24/100: 100%|██████████| 1184/1184 [00:00<00:00, 2362.41it/s]\n",
      "Modular CAFT Epoch 25/100: 100%|██████████| 1184/1184 [00:00<00:00, 2003.81it/s]\n",
      "Modular CAFT Epoch 26/100: 100%|██████████| 1184/1184 [00:00<00:00, 2210.81it/s]\n",
      "Modular CAFT Epoch 27/100: 100%|██████████| 1184/1184 [00:00<00:00, 2395.36it/s]\n",
      "Modular CAFT Epoch 28/100: 100%|██████████| 1184/1184 [00:00<00:00, 2400.80it/s]\n",
      "Modular CAFT Epoch 29/100: 100%|██████████| 1184/1184 [00:00<00:00, 2339.71it/s]\n",
      "Modular CAFT Epoch 30/100: 100%|██████████| 1184/1184 [00:00<00:00, 2377.32it/s]\n",
      "Modular CAFT Epoch 31/100: 100%|██████████| 1184/1184 [00:00<00:00, 2428.29it/s]\n",
      "Modular CAFT Epoch 32/100: 100%|██████████| 1184/1184 [00:00<00:00, 2380.71it/s]\n",
      "Modular CAFT Epoch 33/100: 100%|██████████| 1184/1184 [00:00<00:00, 2471.11it/s]\n",
      "Modular CAFT Epoch 34/100: 100%|██████████| 1184/1184 [00:00<00:00, 2332.60it/s]\n",
      "Modular CAFT Epoch 35/100: 100%|██████████| 1184/1184 [00:00<00:00, 2521.39it/s]\n",
      "Modular CAFT Epoch 36/100: 100%|██████████| 1184/1184 [00:00<00:00, 2432.55it/s]\n",
      "Modular CAFT Epoch 37/100: 100%|██████████| 1184/1184 [00:00<00:00, 2150.80it/s]\n",
      "Modular CAFT Epoch 38/100: 100%|██████████| 1184/1184 [00:00<00:00, 2202.31it/s]\n",
      "Modular CAFT Epoch 39/100: 100%|██████████| 1184/1184 [00:00<00:00, 2224.07it/s]\n",
      "Modular CAFT Epoch 40/100: 100%|██████████| 1184/1184 [00:00<00:00, 2213.59it/s]\n",
      "Modular CAFT Epoch 41/100: 100%|██████████| 1184/1184 [00:00<00:00, 2508.72it/s]\n",
      "Modular CAFT Epoch 42/100: 100%|██████████| 1184/1184 [00:00<00:00, 2233.17it/s]\n",
      "Modular CAFT Epoch 43/100: 100%|██████████| 1184/1184 [00:00<00:00, 2384.07it/s]\n",
      "Modular CAFT Epoch 44/100: 100%|██████████| 1184/1184 [00:00<00:00, 2459.77it/s]\n",
      "Modular CAFT Epoch 45/100: 100%|██████████| 1184/1184 [00:00<00:00, 2502.55it/s]\n",
      "Modular CAFT Epoch 46/100: 100%|██████████| 1184/1184 [00:00<00:00, 2492.74it/s]\n",
      "Modular CAFT Epoch 47/100: 100%|██████████| 1184/1184 [00:00<00:00, 2210.67it/s]\n",
      "Modular CAFT Epoch 48/100: 100%|██████████| 1184/1184 [00:00<00:00, 2385.46it/s]\n",
      "Modular CAFT Epoch 49/100: 100%|██████████| 1184/1184 [00:00<00:00, 2352.79it/s]\n",
      "Modular CAFT Epoch 50/100: 100%|██████████| 1184/1184 [00:00<00:00, 2276.25it/s]\n",
      "Modular CAFT Epoch 51/100: 100%|██████████| 1184/1184 [00:00<00:00, 2475.21it/s]\n",
      "Modular CAFT Epoch 52/100: 100%|██████████| 1184/1184 [00:00<00:00, 2190.38it/s]\n",
      "Modular CAFT Epoch 53/100: 100%|██████████| 1184/1184 [00:00<00:00, 2388.79it/s]\n",
      "Modular CAFT Epoch 54/100: 100%|██████████| 1184/1184 [00:00<00:00, 2368.16it/s]\n",
      "Modular CAFT Epoch 55/100: 100%|██████████| 1184/1184 [00:00<00:00, 2285.82it/s]\n",
      "Modular CAFT Epoch 56/100: 100%|██████████| 1184/1184 [00:00<00:00, 2393.23it/s]\n",
      "Modular CAFT Epoch 57/100: 100%|██████████| 1184/1184 [00:00<00:00, 2103.35it/s]\n",
      "Modular CAFT Epoch 58/100: 100%|██████████| 1184/1184 [00:00<00:00, 2439.43it/s]\n",
      "Modular CAFT Epoch 59/100: 100%|██████████| 1184/1184 [00:00<00:00, 2327.24it/s]\n",
      "Modular CAFT Epoch 60/100: 100%|██████████| 1184/1184 [00:00<00:00, 2489.36it/s]\n",
      "Modular CAFT Epoch 61/100: 100%|██████████| 1184/1184 [00:00<00:00, 2232.04it/s]\n",
      "Modular CAFT Epoch 62/100: 100%|██████████| 1184/1184 [00:00<00:00, 2179.91it/s]\n",
      "Modular CAFT Epoch 63/100: 100%|██████████| 1184/1184 [00:00<00:00, 2503.63it/s]\n",
      "Modular CAFT Epoch 64/100: 100%|██████████| 1184/1184 [00:00<00:00, 2399.09it/s]\n",
      "Modular CAFT Epoch 65/100: 100%|██████████| 1184/1184 [00:00<00:00, 2383.58it/s]\n",
      "Modular CAFT Epoch 66/100: 100%|██████████| 1184/1184 [00:00<00:00, 2440.02it/s]\n",
      "Modular CAFT Epoch 67/100: 100%|██████████| 1184/1184 [00:00<00:00, 1926.30it/s]\n",
      "Modular CAFT Epoch 68/100: 100%|██████████| 1184/1184 [00:00<00:00, 2483.99it/s]\n",
      "Modular CAFT Epoch 69/100: 100%|██████████| 1184/1184 [00:00<00:00, 2336.19it/s]\n",
      "Modular CAFT Epoch 70/100: 100%|██████████| 1184/1184 [00:00<00:00, 2147.47it/s]\n",
      "Modular CAFT Epoch 71/100: 100%|██████████| 1184/1184 [00:00<00:00, 2303.58it/s]\n",
      "Modular CAFT Epoch 72/100: 100%|██████████| 1184/1184 [00:00<00:00, 2330.94it/s]\n",
      "Modular CAFT Epoch 73/100: 100%|██████████| 1184/1184 [00:00<00:00, 2363.17it/s]\n",
      "Modular CAFT Epoch 74/100: 100%|██████████| 1184/1184 [00:00<00:00, 2395.23it/s]\n",
      "Modular CAFT Epoch 75/100: 100%|██████████| 1184/1184 [00:00<00:00, 2423.33it/s]\n",
      "Modular CAFT Epoch 76/100: 100%|██████████| 1184/1184 [00:00<00:00, 2530.87it/s]\n",
      "Modular CAFT Epoch 77/100: 100%|██████████| 1184/1184 [00:00<00:00, 2430.43it/s]\n",
      "Modular CAFT Epoch 78/100: 100%|██████████| 1184/1184 [00:00<00:00, 2463.24it/s]\n",
      "Modular CAFT Epoch 79/100: 100%|██████████| 1184/1184 [00:00<00:00, 2515.93it/s]\n",
      "Modular CAFT Epoch 80/100: 100%|██████████| 1184/1184 [00:00<00:00, 2486.13it/s]\n",
      "Modular CAFT Epoch 81/100: 100%|██████████| 1184/1184 [00:00<00:00, 2179.21it/s]\n",
      "Modular CAFT Epoch 82/100: 100%|██████████| 1184/1184 [00:00<00:00, 2386.78it/s]\n",
      "Modular CAFT Epoch 83/100: 100%|██████████| 1184/1184 [00:00<00:00, 2421.63it/s]\n",
      "Modular CAFT Epoch 84/100: 100%|██████████| 1184/1184 [00:00<00:00, 2413.95it/s]\n",
      "Modular CAFT Epoch 85/100: 100%|██████████| 1184/1184 [00:00<00:00, 2477.64it/s]\n",
      "Modular CAFT Epoch 86/100: 100%|██████████| 1184/1184 [00:00<00:00, 2484.58it/s]\n",
      "Modular CAFT Epoch 87/100: 100%|██████████| 1184/1184 [00:00<00:00, 2328.48it/s]\n",
      "Modular CAFT Epoch 88/100: 100%|██████████| 1184/1184 [00:00<00:00, 2291.09it/s]\n",
      "Modular CAFT Epoch 89/100: 100%|██████████| 1184/1184 [00:00<00:00, 2345.98it/s]\n",
      "Modular CAFT Epoch 90/100: 100%|██████████| 1184/1184 [00:00<00:00, 2485.21it/s]\n",
      "Modular CAFT Epoch 91/100: 100%|██████████| 1184/1184 [00:00<00:00, 2385.77it/s]\n",
      "Modular CAFT Epoch 92/100: 100%|██████████| 1184/1184 [00:00<00:00, 2320.02it/s]\n",
      "Modular CAFT Epoch 93/100: 100%|██████████| 1184/1184 [00:00<00:00, 2487.20it/s]\n",
      "Modular CAFT Epoch 94/100: 100%|██████████| 1184/1184 [00:00<00:00, 2291.61it/s]\n",
      "Modular CAFT Epoch 95/100: 100%|██████████| 1184/1184 [00:00<00:00, 2471.08it/s]\n",
      "Modular CAFT Epoch 96/100: 100%|██████████| 1184/1184 [00:00<00:00, 2401.29it/s]\n",
      "Modular CAFT Epoch 97/100: 100%|██████████| 1184/1184 [00:00<00:00, 2438.87it/s]\n",
      "Modular CAFT Epoch 98/100: 100%|██████████| 1184/1184 [00:00<00:00, 2263.44it/s]\n",
      "Modular CAFT Epoch 99/100: 100%|██████████| 1184/1184 [00:00<00:00, 2428.13it/s]\n",
      "Modular CAFT Epoch 100/100: 100%|██████████| 1184/1184 [00:00<00:00, 1922.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "--- Modular CAFT Results ---\n",
      "Backbone: Custom Hybrid (ResNet + Adapter + Transformer)\n",
      "-------------------------------------------------\n",
      "Caching Phase Time:           109.43 seconds\n",
      "Accelerated Training Time:    50.59 seconds (for 100 epochs)\n",
      "Total Time Elapsed:           160.02 seconds\n",
      "Final Training Accuracy:      52.03%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "68b2f9843eb54ffc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
